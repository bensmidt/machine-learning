\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
% hyper links
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{bm}
% Formatting quotes properly
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}
    %\setlength{\extrarowheight}{1pt}
\usepackage{lipsum}



% \newif\ifanswers
% \answerstrue % comment out to hide answers

% \usepackage[compact]{titlesec}
% \usepackage{fancyhdr} % Required for custom headers
% \usepackage{lastpage} % Required to determine the last page for the footer
% \usepackage{extramarks} % Required for headers and footers
% \usepackage[usenames,dvipsnames]{color} % Required for custom colors
% \usepackage{graphicx} % Required to insert images
% \usepackage{listings} % Required for insertion of code
% \usepackage{courier} % Required for the courier font
% \usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
% \usepackage{enumerate}
% \usepackage{enumitem}
% \usepackage{subfigure}
% \usepackage{booktabs}
% \usepackage{amsmath, amsthm, amssymb}
% \usepackage{caption}
% \usepackage{hyperref}
% \captionsetup[table]{skip=4pt}
% \usepackage{framed}
% \usepackage{bm}
% \usepackage{minted}
% \usepackage{tikz}
% \usetikzlibrary{positioning,patterns,fit}

\begin{document}

\noindent Author: Benjamin Smidt

\noindent Created: October 31st, 2022

\noindent Last Updated: November 2nd, 2022
\begin{center}
\section*{CS 224N A4: Neural Machine Translation with RNNs}
\end{center}

\paragraph{} Note to the reader. This is my work for assignment 4 of Stanford's course
\href{https://web.stanford.edu/class/cs224n/}{CS 224N: Natural Language Processing with Deep Learning}. 
You can find the lecture Winter 2021 lectures series on YouTube 
\href{https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ}{here}.
This document is meant to be used as a reference, explanation, and resource for the assignment, 
not necessarily a comprehensive overview of Word Vectors. If there's a typo or a correction 
needs to be made, feel free to email me at benjamin.smidt@utexas.edu so I can fix it. 
Thank you! I hope you find this document helpful :). 

\tableofcontents{}

\newpage

\section{Neural Machine Translation with RNNs}

\subsection{Problem G}
\underline{Question}:
~\\
The \emph{generate-sent-masks()} function in \emph{nmt-model.py} produces a tensor 
called \emph{enc-masks}. It has shape (batch size, max source sentence length) 
and contains 1s in positions corresponding to ‘pad’ tokens in the input, 
and 0s for non-pad tokens. Look at how the masks are used during the 
attention computation in the step() function.
First explain (in around three sentences) what effect the masks have 
on the entire attention computation. Then explain (in one or two 
sentences) why it is necessary to use the masks in this way.
~\\
~\\
\underline{Answer}: 
~\\
You can see in the code that we set the attention weights for the masked 
values (wherever ‘pad’ token appears) to negative infinity. As a result 
they are given zero weight in the softmax function, meaning the network 
never attends to these words. This is what we want because we don't want 
the model to use the ‘pad’ tokens for prediction since they're just a filler 
value. 
~\\
~\\
We have to use the masks in this manner because we need to keep our 
vector shapes but we don't want to attend the ‘pad’ tokens. By setting 
their weights to negative infinity before the softmax, we effectively 
cut them out of the attention mechanism while retaining the vector shapes 
we need for the network to function properly. 

\subsection{Problem I}

\section{Analyzing NMT Systems}
I will come back to this over Thanksgiving, I need to focus on other things 
at the moment and keep moving through to assignment 5 for some projects 
I'm working on. 

\end{document}