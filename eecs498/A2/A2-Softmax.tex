\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{hyperref}  % hyper links
\usepackage[utf8]{inputenc}
\begin{document}
\noindent Author: Benjamin Smidt

\noindent Created: September 7, 2022

\noindent Last Updated: September 13, 2022
\begin{center}
\section*{Assignment 2: Softmax Classifier}
\end{center}

\noindent Note to the reader: this is my work for assignment two of Michigan's course
EECS 498: Deep Learning for Computer Vision. This document is thoroughly researched but
may not be perfect. If there's a typo or a correction needs to be made, feel free to 
email me at benjamin.smidt@utexas.edu so I can fix it. Thank you! I hope you find this 
document helpful.

\tableofcontents{}

\newpage

\section{Mathematics}
\subsection{Loss}
\subsubsection*{Resources}

\begin{itemize}
    \item \href{https://cs231n.github.io/linear-classify/#svm-vs-softmax}{CS 231N: SVM's and Softmax }
\end{itemize}

\paragraph{}
The equation for the loss function of a single example of 
Multinomial Logistic Regression is:  

\begin{equation}
    L_{i} = -log(\frac{e^{f_{y_{i}}}}{\sum_{j=1}^C e^{f_j}}) 
    = -f_{y_{i}} + log(\sum_{j} e^{f_{j}})
\end{equation}

\begin{equation}
    f_j = f(x_{i}, W)_j = (Wx_{i})_j
\end{equation}

\noindent Thus, to find the loss for the training data, we simply need 
to average the loss $L_{i}$ for each example. For SVM's, the loss was 
called \emph{hinge loss}. The loss for a Softmax Classifier is known as 
\emph{cross-entropy loss}. 

\subsection{Gradient}

\paragraph{}
To find the gradient with respect to our weight 
matrix W, let's again focus on one example. We can rewrite our loss function as:

\begin{equation}
    L_{i} = -log(\frac{e^{W_{y_{i}}x_{i}}}{\sum_{j} e^{W_{j}x_{i}}}) 
\end{equation}

where $W_{y_{i}}$ represents the weights for the correct label for example $i$ and $W_{j}$ 
is the weights for any given class (including $W_{y_{i}}$) for example $i$. 
Note that since the shape 
of $W$ is DxC, each $W_{j}$ is a column of $W$. 

Let's start by reformulating our loss function a bit.

\begin{equation}
    L_{i} = -log(\frac{e^{W_{y_{i}}x_{i}}}{\sum_{j=1}^C e^{W_{j}x_{i}}}) 
\end{equation}
\begin{equation}
    L_{i} = log(\frac{\sum_{j=1}^C e^{W_{j}x_{i}}}{e^{W_{y_{i}}x_{i}}}) 
\end{equation}
\begin{equation}
    L_{i} = log({\sum_{j=1}^C e^{W_{j}x_{i}}}) - W_{y_{i}}x_{i}
\end{equation}

\noindent Then we find the gradient with respect to $W_{y_{i}}$
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{\partial}{\partial W_{y_{i}}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - \frac{\partial}{\partial W_{y_{i}}}W_{y_{i}}x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{\partial}{\partial W_{y_{i}}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{1}{\sum_{j=1}^C e^{W_{j}x_{i}}}
    \frac{\partial}{\partial W_{y_{i}}} 
    (e^{W_{1}x_{i}}+...+e^{W_{y_{i}}x_{i}}+...+e^{W_{C}x_{i}})
    - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = \frac{x_{i}e^{W_{y_{i}}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}} - x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{y_{i}}} = x_{i} (\frac{e^{W_{y_{i}}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}} - 1)
\end{equation}

\noindent The math works out similarly for the gradient with respect to $W_{j}$
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{\partial}{\partial W_{j}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
    - W_{y_{i}}x_{i}
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{\partial}{\partial W_{j}} 
    log({\sum_{j=1}^C e^{W_{j}x_{i}}}) 
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{1}{\sum_{j=1}^C e^{W_{j}x_{i}}}
    \frac{\partial}{\partial W_{j}} 
    (e^{W_{1}x_{i}}+...+e^{W_{j}x_{i}}+...+e^{W_{C}x_{i}})
\end{equation}
\begin{equation}
    \frac{\partial L_{i}}{\partial W_{j}} = \frac{x_{i}e^{W_{j}x_{i}}}
    {\sum_{j=1}^C e^{W_{j}x_{i}}}
\end{equation}

\section{Programming}

\subsection{Softmax Loss Naive}
Per usual with the naive function, this one's again pretty straight forward. Although I did 
implement a hybrid version with some vectorization just because I didn't feel like writing 
out the for loops and all that jazz.

\subsection{Softmax Loss Vectorized}
\paragraph{}
We begin by matrix multiplying $XW$ ($X$ is NxD and $W$ is DxC) to get our scores and store 
them in the matrix $f$. We then normalize the matrix for numerical stability purposes 
by making the largest value 0. For more information see \href{https://cs231n.github.io/linear-classify/#svm-vs-softmax}{CS 231N: SVM and Softmax}. 
We then raise all of our (shifted) scores by $e$ and stored those values in matrix $ef$. 

For the loss function we sum all the scores in a given example for every example. Since 
matrix $ef$ is NxC, we sum over the second dimension $dim = 1$. Finally we grab all the 
scores from the correct classes ($e-yi$ with shape Nx1) and divide each correct score by 
the sum of the all the scores for each example. We average those scores, add our 
regularization term, and viola, loss complete. 

For the gradient, let's first work on the gradient with respect to $w_j$. We begin by cloning
matrix $ef$ (NxC) and setting all the correct class score values to zero. Then we divide all 
the scores by the sum of the scores in the same example and store it in matrix k. Finally, 
we just multiply k.T by X, divide everything by N and add it to the gradient. I can't explain
with words why that multiplication works other than the fact that the shape of k.T is CxN and
X is NxD. Since dW is CxD, matrix multiplying them gives the correct output. If you don't 
understand (I didn't at first), you need to draw out the shapes and trace how the matrix addition
correctly sums elements to produce the proper matrix. 

Now we can computer the gradient with respect to $w_{y_i}$. We use a mask to get rid of all 
the scores that aren't correct scores (set equal to 0). Then we divide the (correct) scores 
by the sum of the scores in the same example for each score. Next we subtract 1 from every 
correct score using the mask we created earlier since we want to leave all the incorrect 
scores as 0 still. Finally we matrix multiply k.T by X again and add it to the gradient. We 
finish off by adding L2 regularization. 

\subsection{Softmax Get Search Parameters}
This function just returns different parameter combinations in seach of the best one. 




\section{References}
\begin{enumerate}
    \item \href{https://cs231n.github.io/linear-classify/#svm-vs-softmax}{CS 231N: SVM's and Softmax }
    \item \href{https://www.youtube.com/watch?v=qcSEP17uKKY&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r}{EECS 498 Lecture 3: Linear Classifiers}
    \item \href{https://www.youtube.com/watch?v=YnQJTfbwBM8&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=4}{EECS 498 Lecture 4: Optimization}
\end{enumerate}

\end{document}