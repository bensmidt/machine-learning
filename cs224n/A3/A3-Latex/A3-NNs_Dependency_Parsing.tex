\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
% hyper links
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{bm}
% Formatting quotes properly
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{array}
    %\setlength{\extrarowheight}{1pt}
\usepackage{lipsum}

\begin{document}

\noindent Author: Benjamin Smidt

\noindent Created: October 17th, 2022

\noindent Last Updated: February 16th, 2023
\begin{center}
\section*{CS 224N A3: Dependency Parsing}
\end{center}

\paragraph{} \emph{Note to the reader}. This is my work for assignment 3 of Stanford's course
\href{https://web.stanford.edu/class/cs224n/}{CS 224N: Natural Language Processing with Deep Learning}. 
You can find the Winter 2021 lectures on YouTube \href{https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ}{here}.
This document is meant to be a reference, explanation, and resource for assignment 3. 
If there's a typo or a error, please email me at benjamin.smidt@utexas.edu so I can fix it. 
Finally, here is a link to my \href{https://github.com/bensmidt/CS224N-DL-NLP}{GitHub repo}. 

\tableofcontents{}

\newpage

\section{Machine Learning and Neural Networks}

\subsection{Adam Optimizer}
In our traditional Stochastic Gradient Descent, the update rule is 
\begin{equation*}
    \theta \leftarrow \theta - \alpha \nabla_{\theta} J_{\text{minibatch}}(\theta)
\end{equation*}
The Adam optimizer modifies SGD such in an effort to improve convergence. The first 
addition is the use of \emph{momentum}. Adam keeps a rolling average of the gradients 
instead of using only the current gradient. 
\begin{equation*}
    m \leftarrow \beta_1 m + (1 - \beta_1) \nabla_\theta J_{\text{minibatch}} (\theta)
\end{equation*}
\begin{equation*}
    \theta \leftarrow \theta - \alpha m
\end{equation*}
(i) \emph{Briefly explain in 2-4 sentences how using $m$ stops the updates from 
varying as much and why this low variance may be helpful to learning, overall.}
~\\
~\\
$m$ is a weighted average between all the previous updates, embedded in $m$, and the current 
update $\nabla_\theta J_{\text{minibatch}}(\theta)$ (our $\beta_1$ parameter specifies the 
weight to give each term, $\beta_1 = 0.9$ is common). By keeping this weighted average, 
the update naturally gives higher weight to updating in directions that have been 
consistent while updates along dimensions that keep switching between positive and negative 
are given close to no weight. This improves optimization since our updates will minimize steps 
in dimensions that aren't getting us anywhere meaningful (flipping between positive and negative, 
can't decide which direction to go in) and maximize steps in dimensions that are getting us 
somewhere meaningful (nearly all updates have had this direction). 
~\\
~\\
A second addition to Adam is \emph{adaptive learning rates}, which keeps track of $v$, a rolling average
of the magnitude of the gradients. 
\begin{equation*}
    m \leftarrow \beta_1 m + (1 - \beta_1) \nabla_\theta J_{\text{minibatch}}(\theta)
\end{equation*}
\begin{equation*}
    v \leftarrow \beta_2 v + (1 - \beta_2) (\nabla_\theta J_{\text{minibatch}}(\theta) 
    \; \odot \; \nabla_\theta J_{\text{minibatch}}(\theta) )
\end{equation*}
\begin{equation*}
    \theta \leftarrow \theta - \alpha m / \sqrt{v}
\end{equation*}
$odot$ is elementwise multiplication and $/$ is elementwise division. $\beta_2$ is our second 
hyperparameter (often set to 0.99). 
~\\
~\\
(ii) \emph{Since Adam divides the update by $\sqrt{v}$, which of the model parameter will get larger updates? 
Why might this help with learning?}
~\\
~\\
If $v$ is quite small, then dividing by $\sqrt{v}$ will make the term $\alpha m / \sqrt{v}$ large.
This improve learning because often times we need a large learning rate if our gradient update is naturally
small. 
~\\
~\\
The vice versa is also true. When the gradient is very large ($v$ is very large), then we don't
need a very large learning rate and often a small learning rate will be better. In this 
case, by dividing by $\sqrt{v}$, we actually reduce the magnitude of the update to counterbalance
the already large gradient. 

\subsection{Dropout}

Dropout is a form of regularization wherein we "drop" random connections within the 
hidden layers of our network during each update (different connections are 
dropped for each update). We do this mathematically with the following
\begin{equation*}
    h_{\text{drop}} = \gamma d \odot h 
\end{equation*}
where $h$ is a hidden layer, $d \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $h$) is a mask vector
with each entry being 0 (with probability $p_{\text{drop}}$) or 1 (with probability 
$1 - p_{\text{drop}}$), and $\gamma$ is a constant chosen such that the expected 
value of $h_{\text{drop}}$ is $h$
\begin{equation*}
    \mathbb{E}_{p_{\text{drop}}} [h_{\text{drop}}]_i = h_i \; \; \; \forall \; i \in \{1, \dotso, D_h\}
\end{equation*}
(i) \emph{What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or 
show your math derivation using the equations given above}

\begin{equation*}
    \mathbb{E}_{p_{\text{drop}}} [h_{\text{drop}}] = h
\end{equation*}
\begin{equation*}
    \mathbb{E}_{p_{\text{drop}}} [\gamma d \odot h ] = h
\end{equation*}
\begin{equation*}
    \gamma \; \mathbb{E}_{p_{\text{drop}}} [d \odot h] = h
\end{equation*}
\begin{equation*}
    \gamma [h p_{\text{drop}} + (1 - p_{\text{drop}})0] = h
\end{equation*}
\begin{equation*}
    \gamma \; h p_{\text{drop}} = h
\end{equation*}
\begin{equation*}
    \gamma = \frac{1}{p_{\text{drop}}}
\end{equation*}

(ii) \emph{Why should dropout be applied during training? Why should dropout 
NOT be applied during evaluation?}
~\\
~\\
Dropout should be applied during training so the network learns different
pathways that lead to the same prediction. By closing different connections 
randomly, the network is forced to produce multiple paths in which data can flow 
to achieve the correct prediction, theoretically making it more robust. 
~\\
~\\
We wouldn't want to apply dropout during evaluation however because our 
results would be non-deterministic. Due to the randomness of the 
dopout connections, it's possible (and may even be likely depending on the 
network) that evaluating the same input twice yields different predictions. 
Obviously this is an undesirable trait to have in a machine learning model 
so we don't apply dropout during evaluation. 


\section{Neural Transition-Based Dependency Parsing}

\subsection{Problem A}

\bigskip
\scriptsize
\addtolength{\tabcolsep}{-1pt}
\begin{tabular}{l|l|c|c}
    Stack & Buffer & New Dependency & Transition \\
    \hline
    ROOT & I, parsed, this sentence, correctly && Initial Configuration \\
    ROOT, I & parsed, this sentence, correctly && SHIFT \\
    ROOT, I, parsed & this sentence, correctly && SHIFT \\
    ROOT, parsed & this sentence, correctly & parsed $\rightarrow$ I & LEFT-ARC \\
    ROOT, parsed, this & sentence, correctly  && SHIFT \\
    ROOT, parsed, this, sentence & correctly && SHIFT \\
    ROOT, parsed, sentence & correctly & sentence $\rightarrow$ this & LEFT-ARC \\
    ROOT, parsed & correctly & parsed $\rightarrow$ sentence & RIGHT-ARC \\
    ROOT, parsed, correctly &&& SHIFT \\
    ROOT, parsed && parsed $\rightarrow$ correctly & RIGHT-ARC \\
    ROOT && ROOT $\rightarrow$ parsed & RIGHT-ARC \\
\end{tabular}

\normalsize
\subsection{Problem B}
There are only two options at a given step: push a word onto the stack or pop 
a word (and create a dependency) off of the stack. Since every word must be 
pushed onto the stack a single time and popped off the stack a single time, 
this leaves us with $2n$ steps. This is concurrent with our table which has 
11 rows: $2(5) + 1$ where $n = 5$ and we have an additional row for our 
initial state of the stack with only ROOT. 

\subsection{Problem C: Init and Parse Step}
The code here is pretty calm (just see mine if you don't know how to do it, 
it's just initializing the stack, buffer, and dependencies). 
The only important thing to remember is 
that we do NOT want to modify the sentence we're parsing so we must make 
a COPY of the sentence when initializing the buffer. Otherwise, the buffer 
will point to the same memory location as the sentence and we'll be modifying 
our original sentence, which we don't want. 

\subsection{Problem D: Minibatch}
Kind of the same deal as part C, the pseudo code is given so we just have to work out 
the actual implementation. Just see the code for how this is done if you 
can't figure it out (don't forget to use the right data structures!), the instructions 
explain high level what's happening already. 

\subsection{Problem E: Training and Test}
\subsubsection{init}
Just follow the given comments to initialize the proper matrices using
PyTorch with Xavier initialization. 

\subsubsection{embedding-lookup}
This one is a little more fun. My hint is to use \textbf{torch.flatten()} to make the 
embedding lookup simpler (this is one way to do it but I'm sure there are many others). 
Again, if you can't get it, see the PyTorch documentation 
or just take a gander at my code. 

\subsubsection{forward}
To implement the forward function, just use the matrices we already defined and follow
the neural network architecture defined in the instructions. There's many different 
implementations (but I will say I do have a particular affinity for using NumPy's 
\textbf{@} operator for matrix multiplication). 

\subsubsection{train-for-epoch and train}
Honestly, the answer is kind of given between the given comments and PyTorch's 
documentation for using optimizers, loss functions, and updating gradients. 
See the code for the solution but you should be able to figure this out yourself. 
~\\
~\\

\noindent{} My final UAS score was 89.14 with an average training loss of 0.0573028026267612 
at epoch 10. 



\end{document}